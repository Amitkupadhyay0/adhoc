from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel

# Load the model and processor
device = "cuda" if torch.cuda.is_available() else "cpu"
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Load image
image = Image.open("your_image.jpg")

# Define class prompts
texts = ["A medical document, such as a prescription or lab report",
         "A non-medical document, such as an invoice, letter, or receipt"]

# Prepare inputs
inputs = processor(text=texts, images=image, return_tensors="pt", padding=True).to(device)

# Get model outputs
with torch.no_grad():
    outputs = model(**inputs)
    image_features = outputs.image_embeds
    text_features = outputs.text_embeds

    # Normalize
    image_features /= image_features.norm(dim=-1, keepdim=True)
    text_features /= text_features.norm(dim=-1, keepdim=True)

    # Similarity
    similarity = (image_features @ text_features.T).softmax(dim=-1)
    print(similarity)  # Probabilities for each class