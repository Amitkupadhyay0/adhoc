from transformers import AutoProcessor, VisionEncoderDecoderModel
from PIL import Image
import torch

# -------------------------------
# 1. Load PARSeq model + processor
# -------------------------------
model_id = "vkurtin/parseq"   # Hugging Face PARSeq model

processor = AutoProcessor.from_pretrained(model_id)
model = VisionEncoderDecoderModel.from_pretrained(model_id).to("cuda" if torch.cuda.is_available() else "cpu")

# -------------------------------
# 2. Load handwritten image
# -------------------------------
image_path = "handwritten_sample.jpg"  # replace with your image path
image = Image.open(image_path).convert("RGB")

# -------------------------------
# 3. Preprocess image
# -------------------------------
inputs = processor(images=image, return_tensors="pt").to(model.device)

# -------------------------------
# 4. Run inference
# -------------------------------
with torch.no_grad():
    generated_ids = model.generate(**inputs, max_new_tokens=128)

# -------------------------------
# 5. Decode text
# -------------------------------
text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print("üìù Handwritten OCR result:", text)